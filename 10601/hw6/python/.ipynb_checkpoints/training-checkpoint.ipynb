{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import scipy.io\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(fullset=True):\n",
    "  \"\"\"Load mnist dataset\n",
    "\n",
    "  Args:\n",
    "    fullset: whether use full MNIST or not\n",
    "\n",
    "  Returns:\n",
    "    x: shape = (784, samples)\n",
    "    y: shape = (samples,)\n",
    "  \"\"\"\n",
    "  mnist_dataset = scipy.io.loadmat(os.path.join('..','mnist_all'))\n",
    "\n",
    "  # create train and test arrays and labels\n",
    "  xtrain = np.vstack([mnist_dataset['train'+str(i)] for i in range(10)])\n",
    "  xtest = np.vstack([mnist_dataset['test'+str(i)] for i in range(10)])\n",
    "  ytrain = np.hstack([[i for _ in range(mnist_dataset['train'+str(i)].shape[0])]\n",
    "                      for i in range(10)])\n",
    "  ytest = np.hstack([[i for _ in range(mnist_dataset['test'+str(i)].shape[0])]\n",
    "                     for i in range(10)])\n",
    "\n",
    "  # normalize\n",
    "  xtrain = xtrain.astype(np.double) / 255.0\n",
    "  xtest = xtest.astype(np.double) / 255.0\n",
    "\n",
    "  # random shuffle\n",
    "  np.random.seed(100000)\n",
    "  train_indices = list(range(xtrain.shape[0]))\n",
    "  np.random.shuffle(train_indices)\n",
    "  xtrain.take(train_indices, axis=0, out=xtrain)\n",
    "  ytrain.take(train_indices, axis=0, out=ytrain)\n",
    "\n",
    "  test_indices = list(range(xtest.shape[0]))\n",
    "  np.random.shuffle(test_indices)\n",
    "  xtest.take(test_indices, axis=0, out=xtest)\n",
    "  ytest.take(test_indices, axis=0, out=ytest)\n",
    "\n",
    "  # get validation set\n",
    "  m_validate = 10000\n",
    "  xvalidate = xtrain[0:m_validate, :]\n",
    "  yvalidate = ytrain[0:m_validate]\n",
    "  xtrain = xtrain[m_validate:, :]\n",
    "  ytrain = ytrain[m_validate:]\n",
    "  m_train = xtrain.shape[0]\n",
    "  m_test = xtest.shape[0]\n",
    "\n",
    "  # transpose feature matrices so columns are instances and rows are features\n",
    "  xtrain = xtrain.T\n",
    "  xtest = xtest.T\n",
    "  xvalidate = xvalidate.T\n",
    "\n",
    "  # create smaller set for testing purposes\n",
    "  if not fullset:\n",
    "    m_train_small = m_train/20\n",
    "    m_validate_small = m_validate/20\n",
    "    xtrain = xtrain[:, 0:m_train_small]\n",
    "    ytrain = ytrain[0:m_train_small]\n",
    "    xvalidate = xvalidate[:, 0:m_validate_small]\n",
    "    yvalidate = yvalidate[0:m_validate_small]\n",
    "\n",
    "  return [xtrain, ytrain, xvalidate, yvalidate, xtest, ytest]\n",
    "\n",
    "\n",
    "def init_convnet(layers):\n",
    "  \"\"\"Initialize parameters of each layer in LeNet\n",
    "\n",
    "  Args:\n",
    "    layers: a dictionary that defines LeNet\n",
    "\n",
    "  Returns:\n",
    "    params: a dictionary stores initialized parameters\n",
    "  \"\"\"\n",
    "\n",
    "  params = {}\n",
    "\n",
    "  h = layers[1]['height']\n",
    "  w = layers[1]['width']\n",
    "  c = layers[1]['channel']\n",
    "\n",
    "  # Starts with second layer, first layer is data layer\n",
    "  np.random.seed(100000)\n",
    "  for i in range(2, len(layers)+1):\n",
    "    params[i-1] = {}\n",
    "    if layers[i]['type'] == 'CONV':\n",
    "      scale = np.sqrt(3./(h*w*c))\n",
    "      weight = np.random.rand(\n",
    "        layers[i]['k'] * layers[i]['k'] * c / layers[i]['group'],\n",
    "        layers[i]['num']\n",
    "      )\n",
    "\n",
    "      params[i-1]['w'] = 2*scale*weight - scale\n",
    "      params[i-1]['b'] = np.zeros(layers[i]['num'])\n",
    "      # update h, w and c, used in next layer\n",
    "      h = (h + 2*layers[i]['pad'] - layers[i]['k']) / layers[i]['stride'] + 1\n",
    "      w = (w + 2*layers[i]['pad'] - layers[i]['k']) / layers[i]['stride'] + 1\n",
    "      c = layers[i]['num']\n",
    "    elif layers[i]['type'] == 'POOLING':\n",
    "      h = (h - layers[i]['k']) / layers[i]['stride'] + 1\n",
    "      w = (w - layers[i]['k']) / layers[i]['stride'] + 1\n",
    "      params[i-1]['w'] = np.array([])\n",
    "      params[i-1]['b'] = np.array([])\n",
    "    elif layers[i]['type'] == 'IP' and layers[i]['init_type'] == 'gaussian':\n",
    "      scale = np.sqrt(3./(h*w*c))\n",
    "      params[i-1]['w'] = scale*np.random.randn(h*w*c, layers[i]['num'])\n",
    "      params[i-1]['b'] = np.zeros(layers[i]['num'])\n",
    "      h = 1\n",
    "      w = 1\n",
    "      c = layers[i]['num']\n",
    "    elif layers[i]['type'] == 'IP' and layers[i]['init_type'] == 'uniform':\n",
    "      scale = np.sqrt(3./(h*w*c))\n",
    "      params[i-1]['w'] = 2*scale*np.random.rand(h*w*c, layers[i]['num']) - scale\n",
    "      params[i-1]['b'] = np.zeros(layers[i]['num'])\n",
    "      h = 1\n",
    "      w = 1\n",
    "      c = layers[i]['num']\n",
    "    elif layers[i]['type'] == 'RELU':\n",
    "      params[i-1]['w'] = np.array([])\n",
    "      params[i-1]['b'] = np.array([])\n",
    "    elif layers[i]['type'] == 'ELU':\n",
    "      params[i-1]['w'] = np.array([])\n",
    "      params[i-1]['b'] = np.array([])\n",
    "    elif layers[i]['type'] == 'LOSS':\n",
    "      scale = np.sqrt(3./(h*w*c))\n",
    "      num = layers[i]['num']\n",
    "      # last layer is K-1\n",
    "      params[i-1]['w'] = 2*scale*np.random.rand(h*w*c, num-1) - scale\n",
    "      params[i-1]['b'] = np.zeros(num - 1)\n",
    "      h = 1\n",
    "      w = 1\n",
    "      c = layers[i]['num']\n",
    "  return params\n",
    "\n",
    "\n",
    "def conv_net(params, layers, data, labels, doBackProp):\n",
    "  \"\"\"\n",
    "\n",
    "  Args:\n",
    "    params: a dictionary that stores hyper parameters\n",
    "    layers: a dictionary that defines LeNet\n",
    "    data: input data with shape (784, batch size)\n",
    "    labels: label with shape (batch size,)\n",
    "    doBackProp: enables backprop when set to True\n",
    "\n",
    "  Returns:\n",
    "    cp: cost\n",
    "    param_grad: gradients w.r.t all parameters across all layers\n",
    "  \n",
    "  Defines the CNN. It takes the configuration of the\n",
    "  network structure (defined in layers), the parameters of each layer\n",
    "  (param), the input data (data) and label (labels) and\n",
    "  does feed forward and backward propagation, returns the cost (cp) and\n",
    "  gradient w.r.t all the parameters (param_grad).\n",
    "  \n",
    "  ########################## IMPORTANT ###################################\n",
    "  WHEN THIS FUNCTION IS CALLED WITH ONLY ONE OUTPUT ARGUMENT, \n",
    "  IT PERFORMS ONLY FORWARD PROPAGATION\n",
    "  WHEN IT IS CALLED WITH TWO OR MORE OUTPUT ARGUMENTS, \n",
    "  IT PERFORMS BOTH FORWARD AND BACKWARD PROPAGATION\n",
    "  \"\"\"\n",
    "  l = len(layers)\n",
    "  batch_size = layers[1]['batch_size']\n",
    "  assert layers[1]['type'] == 'DATA', 'first layer must be data layer'\n",
    "\n",
    "  param_grad = {}\n",
    "  cp = {}\n",
    "  output = {}\n",
    "  output[1] = {}\n",
    "  output[1]['data'] = data\n",
    "  output[1]['height'] = layers[1]['height']\n",
    "  output[1]['width'] = layers[1]['width']\n",
    "  output[1]['channel'] = layers[1]['channel']\n",
    "  output[1]['batch_size'] = layers[1]['batch_size']\n",
    "  output[1]['diff'] = 0\n",
    "\n",
    "  for i in range(2, l):\n",
    "    if layers[i]['type'] == 'CONV':\n",
    "      output[i] = conv_layer_forward(output[i-1], layers[i], params[i-1])\n",
    "    elif layers[i]['type'] == 'POOLING':\n",
    "      output[i] = pooling_layer_forward(output[i-1], layers[i])\n",
    "    elif layers[i]['type'] == 'IP':\n",
    "      output[i] = inner_product_forward(output[i-1], layers[i], params[i-1])\n",
    "    elif layers[i]['type'] == 'RELU':\n",
    "      output[i] = relu_forward(output[i-1], layers[i])\n",
    "\n",
    "  i = l\n",
    "  assert layers[i]['type'] == 'LOSS', 'last layer must be loss layer'\n",
    "\n",
    "  wb = np.vstack([params[i-1]['w'], params[i-1]['b']])\n",
    "  [cost, grad, input_od, percent] = mlrloss(wb,\n",
    "                                            output[i-1]['data'],\n",
    "                                            labels,\n",
    "                                            layers[i]['num'], 1)\n",
    "\n",
    "  param_grad[i-1] = {}\n",
    "  param_grad[i-1]['w'] = grad[0:-1, :]\n",
    "  param_grad[i-1]['b'] = grad[-1, :]\n",
    "  param_grad[i-1]['w'] = param_grad[i-1]['w'] / batch_size\n",
    "  param_grad[i-1]['b'] = param_grad[i-1]['b'] / batch_size\n",
    "\n",
    "  cp['cost'] = cost/batch_size\n",
    "  cp['percent'] = percent\n",
    "\n",
    "  if doBackProp:\n",
    "    # range: [l-1, 2]\n",
    "    for i in range(l-1,1,-1):\n",
    "      param_grad[i-1] = {}\n",
    "\n",
    "      if layers[i]['type'] == 'CONV':\n",
    "        output[i]['diff'] = input_od\n",
    "        param_grad[i-1], input_od = conv_layer_backward(output[i],\n",
    "                                                      output[i-1],\n",
    "                                                      layers[i],\n",
    "                                                      params[i-1])\n",
    "      elif layers[i]['type'] == 'POOLING':\n",
    "        output[i]['diff'] = input_od\n",
    "        input_od = pooling_layer_backward(output[i],\n",
    "                                        output[i-1],\n",
    "                                        layers[i])\n",
    "        param_grad[i-1]['w'] = np.array([])\n",
    "        param_grad[i-1]['b'] = np.array([])\n",
    "      elif layers[i]['type'] == 'IP':\n",
    "        output[i]['diff'] = input_od\n",
    "        param_grad[i-1], input_od = inner_product_backward(output[i],\n",
    "                                                         output[i-1],\n",
    "                                                         layers[i],\n",
    "                                                         params[i-1])\n",
    "      elif layers[i]['type'] == 'RELU':\n",
    "        output[i]['diff'] = input_od\n",
    "        input_od = relu_backward(output[i], output[i-1], layers[i])\n",
    "        param_grad[i-1]['w'] = np.array([])\n",
    "        param_grad[i-1]['b'] = np.array([])\n",
    "\n",
    "      param_grad[i-1]['w'] = param_grad[i-1]['w'] / batch_size\n",
    "      param_grad[i-1]['b'] = param_grad[i-1]['b'] / batch_size\n",
    "\n",
    "  return output, cp, param_grad\n",
    "\n",
    "\n",
    "def im2col_conv(input_n, layer, h_out, w_out):\n",
    "  \"\"\"Convert columns to image\n",
    "\n",
    "  Args:\n",
    "    input_n: input data, shape=(h_in*w_in*c, )\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "    h_out: output height\n",
    "    w_out: output width\n",
    "\n",
    "  Returns:\n",
    "    col: shape=(k*k*c, h_out*w_out)\n",
    "  \"\"\"\n",
    "  h_in = input_n['height']\n",
    "  w_in = input_n['width']\n",
    "  c = input_n['channel']\n",
    "  k = layer['k']\n",
    "  stride = layer['stride']\n",
    "  pad = layer['pad']\n",
    "\n",
    "  im = np.reshape(input_n['data'], (h_in, w_in, c))\n",
    "  im = np.pad(im, ((pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=(0, 0))\n",
    "  col = np.zeros((k*k*c, h_out*w_out))\n",
    "\n",
    "  for h in range(h_out):\n",
    "    for w in range(w_out):\n",
    "      matrix_hw = im[h*stride: h*stride+k, w*stride: w*stride+k, :]\n",
    "      col[:, h*w_out + w] = matrix_hw.flatten()\n",
    "\n",
    "  return col\n",
    "\n",
    "\n",
    "def conv_layer_forward(input, layer, param):\n",
    "  \"\"\"Convolution forward\n",
    "\n",
    "  Args:\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "    param: parameters, a dictionary\n",
    "\n",
    "  Returns:\n",
    "    output: a dictionary contains output data and shape information\n",
    "  \"\"\"\n",
    "  # get parameters\n",
    "  h_in = input['height']\n",
    "  w_in = input['width']\n",
    "  c = input['channel']\n",
    "  batch_size = input['batch_size']\n",
    "  k = layer['k']\n",
    "  pad = layer['pad']\n",
    "  stride = layer['stride']\n",
    "  group = layer['group']\n",
    "  num = layer['num']\n",
    "\n",
    "  # resolve output shape\n",
    "  h_out = (h_in + 2*pad - k) / stride + 1\n",
    "  w_out = (w_in + 2*pad - k) / stride + 1\n",
    "\n",
    "  assert h_out == np.floor(h_out), 'h_out is not integer (error occured at convolution forward)'\n",
    "  assert w_out == np.floor(w_out), 'w_out is not integer (error occured at convolution forward)'\n",
    "  assert np.mod(c, group) == 0, 'c is not multiple of group (error occured at convolution forward)'\n",
    "  assert np.mod(num, group) == 0, 'c is not multiple of group (error occured at convolution forward)'\n",
    "\n",
    "  # set output shape\n",
    "  output = {}\n",
    "  output['height'] = h_out\n",
    "  output['width'] = w_out\n",
    "  output['channel'] = num\n",
    "  output['batch_size'] = batch_size\n",
    "  output['data'] = np.zeros((h_out * w_out * num, batch_size))\n",
    "\n",
    "  input_n = {\n",
    "    'height': h_in,\n",
    "    'width': w_in,\n",
    "    'channel': c,\n",
    "  }\n",
    "  for n in range(batch_size):\n",
    "    input_n['data'] = input['data'][:, n]\n",
    "    col = im2col_conv(input_n, layer, h_out, w_out)\n",
    "    tmp_output = col.T.dot(param['w']) + param['b']\n",
    "    output['data'][:, n] = tmp_output.flatten()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def col2im_conv(col, input, layer, h_out, w_out):\n",
    "  \"\"\"Convert image to columns\n",
    "\n",
    "  Args:\n",
    "    col: shape = (k*k, c, h_out*w_out)\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "    h_out: output height\n",
    "    w_out: output width\n",
    "\n",
    "  Returns:\n",
    "    im: shape = (h_in, w_in, c)\n",
    "  \"\"\"\n",
    "  h_in = input['height']\n",
    "  w_in = input['width']\n",
    "  c = input['channel']\n",
    "  k = layer['k']\n",
    "  stride = layer['stride']\n",
    "\n",
    "  im = np.zeros((h_in, w_in, c))\n",
    "  col = np.reshape(col, (k*k*c, h_out*w_out))\n",
    "  for h in range(h_out):\n",
    "    for w in range(w_out):\n",
    "      im[h*stride: h*stride+k, w*stride: w*stride+k, :] = \\\n",
    "        im[h*stride: h*stride+k, w*stride: w*stride+k, :] + \\\n",
    "        np.reshape(col[:, h*w_out + w], (k, k, c))\n",
    "\n",
    "  return im\n",
    "\n",
    "\n",
    "def conv_layer_backward(output, input, layer, param):\n",
    "  \"\"\"Convolution backward\n",
    "\n",
    "  Args:\n",
    "    output: a dictionary contains output data and shape information\n",
    "    input: a dictionary contains output data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "    param: parameters, a dictionary\n",
    "\n",
    "  Returns:\n",
    "    para_grad: a dictionary stores gradients of parameters\n",
    "    input_od: gradients w.r.t input data\n",
    "  \"\"\"\n",
    "\n",
    "  # get parameters\n",
    "  h_in = input['height']\n",
    "  w_in = input['width']\n",
    "  c = input['channel']\n",
    "  batch_size = input['batch_size']\n",
    "  k = layer['k']\n",
    "  num = layer['num']\n",
    "\n",
    "  h_out = output['height']\n",
    "  w_out = output['width']\n",
    "\n",
    "  input_od = np.zeros(input['data'].shape)\n",
    "  param_grad = {}\n",
    "  param_grad['b'] = np.zeros(param['b'].shape)\n",
    "  param_grad['w'] = np.zeros(param['w'].shape)\n",
    "\n",
    "  input_n = {\n",
    "    'height': h_in,\n",
    "    'width': w_in,\n",
    "    'channel': c,\n",
    "  }\n",
    "  for n in range(batch_size):\n",
    "    input_n['data'] = input['data'][:, n]\n",
    "    col = im2col_conv(input_n, layer, h_out, w_out)\n",
    "    tmp_data_diff = np.reshape(output['diff'][:, n], (h_out*w_out, num))\n",
    "\n",
    "    param_grad['b'] += np.sum(tmp_data_diff)\n",
    "    param_grad['w'] += col.dot(tmp_data_diff)\n",
    "    col_diff = param['w'].dot(tmp_data_diff.T)\n",
    "\n",
    "    im = col2im_conv(col_diff, input, layer, h_out, w_out)\n",
    "    input_od[:, n] = im.flatten()\n",
    "\n",
    "  return param_grad, input_od\n",
    "\n",
    "\n",
    "def pooling_layer_forward(input, layer):\n",
    "  \"\"\"Pooling forward\n",
    "\n",
    "  Args:\n",
    "    input: a dictionary contains output data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "\n",
    "  Returns:\n",
    "    output: a dictionary contains output data and shape information\n",
    "  \"\"\"\n",
    "  h_in = input['height']\n",
    "  w_in = input['width']\n",
    "  c = input['channel']\n",
    "  batch_size = input['batch_size']\n",
    "  k = layer['k']\n",
    "  pad = layer['pad']\n",
    "  stride = layer['stride']\n",
    "\n",
    "  h_out = (h_in + 2*pad - k) / stride + 1\n",
    "  w_out = (w_in + 2*pad - k) / stride + 1\n",
    "\n",
    "  assert h_out == np.floor(h_out), 'h_out is not integer (error occured at pooling forward)'\n",
    "  assert w_out == np.floor(w_out), 'w_out is not integer (error occured at pooling forward)'\n",
    "\n",
    "  output = {}\n",
    "  output['height'] = h_out\n",
    "  output['width'] = w_out\n",
    "  output['channel'] = c\n",
    "  output['batch_size'] = batch_size\n",
    "  output['data'] = np.zeros((h_out * w_out * c, batch_size))\n",
    "  # print(\"h_out is: {}, w_out is: {}, c is: {} and k is:{}\".format(h_out, w_out, c, k))\n",
    "\n",
    "  # TODO: implement your pooling forward here\n",
    "  # implementation begins\n",
    "  # data = copy(output['data'])\n",
    "  # data.shape = [c,w_out,h_out,batch_size]\n",
    "  input_n = {\n",
    "    'height': h_in,\n",
    "    'width': w_in,\n",
    "    'channel': c,\n",
    "  }\n",
    "  # for batch in range(batch_size):\n",
    "  #   input_n['data'] = input['data'][:, batch]\n",
    "  #   col = im2col_conv(input_n, layer, h_out, w_out)\n",
    "  #   col.shape = [k*k,c,h_out*w_out]\n",
    "  #   # print(col.shape)\n",
    "  #   # print(output['data'].shape)\n",
    "  #   for channel in range(c):\n",
    "  #     # print(channel*h_out*w_out, (channel+1)*h_out*w_out, channel*k*k, (channel+1)*k*k)\n",
    "  #     output['data'][channel*h_out*w_out:(channel+1)*h_out*w_out, batch] = np.max(col[:,channel,:], axis=0)\n",
    "  \n",
    "  output['data'].shape = [h_out * w_out, c, batch_size]\n",
    "\n",
    "  for batch in range(batch_size):\n",
    "    input_n['data'] = input['data'][:, batch]\n",
    "    col = im2col_conv(input_n, layer, h_out, w_out)\n",
    "    col.shape = [k*k,c,h_out*w_out]\n",
    "    # print(col.shape)\n",
    "    # print(output['data'].shape)\n",
    "    for channel in range(c):\n",
    "      # print(channel*h_out*w_out, (channel+1)*h_out*w_out, channel*k*k, (channel+1)*k*k)\n",
    "      output['data'][:,channel, batch] = np.max(col[:,channel,:], axis=0)\n",
    "  output['data'].shape = [h_out * w_out * c, batch_size]\n",
    "  # implementation ends\n",
    "  assert np.all(output['data'].shape == (h_out * w_out * c, batch_size)), 'output[\\'data\\'] has incorrect shape! (error occured at pooling forward)'\n",
    "  return output\n",
    "\n",
    "\n",
    "def pooling_layer_backward(output, input, layer):\n",
    "  \"\"\"Pooling backward\n",
    "\n",
    "  Args:\n",
    "    output: a dictionary contains output data and shape information\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "\n",
    "  Returns:\n",
    "    input_od: gradients w.r.t input data\n",
    "  \"\"\"\n",
    "  c = input['channel']\n",
    "  batch_size = input['batch_size']\n",
    "  h_out = output['height']\n",
    "  w_out = output['width']\n",
    "  k = layer['k']\n",
    "\n",
    "  input_od = np.zeros(input['data'].shape)\n",
    "\n",
    "  input_i = {\n",
    "    'height': input['height'],\n",
    "    'width': input['width'],\n",
    "    'channel': c,\n",
    "  }\n",
    "  for i in range(batch_size):\n",
    "    tmp_diff = np.reshape(output['diff'][:, i], (h_out*w_out, c))\n",
    "    tmp_diff = np.reshape(tmp_diff.T, (1, c, h_out, w_out))\n",
    "    tmp_diff = np.tile(tmp_diff, (k*k, 1, 1, 1))\n",
    "\n",
    "    tmp = np.reshape(output['data'][:, i], (h_out*w_out, c))\n",
    "    tmp = np.reshape(tmp.T, (1, c, h_out, w_out))\n",
    "    tmp = np.tile(tmp, (k*k, 1, 1, 1))\n",
    "\n",
    "    input_i['data'] = input['data'][:, i]\n",
    "    col = im2col_conv(input_i, layer, h_out, w_out)\n",
    "    col = np.reshape(col, (k*k, c, h_out, w_out))\n",
    "\n",
    "    col_diff = np.zeros(tmp_diff.shape)\n",
    "    # idx = np.where(tmp == col)\n",
    "    idx = np.where(np.abs(tmp - col) < np.finfo(np.float).eps)\n",
    "    col_diff[idx] = tmp_diff[idx]\n",
    "\n",
    "    im = col2im_conv(col_diff, input, layer, h_out, w_out)\n",
    "    input_od[:, i] = im.flatten()\n",
    "  assert np.all(input['data'].shape == input_od.shape), 'input_od has incorrect shape! (error at pooling backward)'\n",
    "\n",
    "  return input_od\n",
    "\n",
    "\n",
    "def relu_forward(input, layer):\n",
    "  \"\"\"RELU foward\n",
    "\n",
    "  Args:\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "\n",
    "  Returns:\n",
    "    output: a dictionary contains output data and shape information\n",
    "  \"\"\"\n",
    "  output = {}\n",
    "  output['height'] = input['height']\n",
    "  output['width'] = input['width']\n",
    "  output['channel'] = input['channel']\n",
    "  output['batch_size'] = input['batch_size']\n",
    "  output['data'] = np.zeros(input['data'].shape)\n",
    "\n",
    "  # TODO: implement your relu forward pass here\n",
    "  # implementation begins\n",
    "  positive_indeces = input['data'] > 0\n",
    "  output['data'][positive_indeces] = input['data'][positive_indeces]\n",
    "  # implementation ends\n",
    "\n",
    "  assert np.all(output['data'].shape == input['data'].shape), 'output[\\'data\\'] has incorrect shape! (error at relu forward)'\n",
    "  return output\n",
    "\n",
    "\n",
    "def relu_backward(output, input, layer):\n",
    "  \"\"\"RELU backward\n",
    "\n",
    "  Args:\n",
    "    output: a dictionary contains output data and shape information\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "\n",
    "  Returns:\n",
    "    input_od: gradients w.r.t input data\n",
    "  \"\"\"\n",
    "  input_od = np.zeros(input['data'].shape)\n",
    "\n",
    "  idx = np.where(input['data'] > 0)\n",
    "  input_od[idx] = output['diff'][idx]\n",
    "  assert np.all(input['data'].shape == input_od.shape), 'input_od has incorrect shape! (error at relu backward)'\n",
    "\n",
    "  return input_od\n",
    "\n",
    "\n",
    "def inner_product_forward(input, layer, param):\n",
    "  \"\"\"Fully connected layer forward\n",
    "\n",
    "  Args:\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "    param: parameters, a dictionary\n",
    "\n",
    "  Returns:\n",
    "    output: a dictionary contains output data and shape information\n",
    "  \"\"\"\n",
    "  num = layer['num']\n",
    "  batch_size = input['batch_size']\n",
    "\n",
    "  output = {}\n",
    "  output['height'] = 1\n",
    "  output['width'] = 1\n",
    "  output['channel'] = num\n",
    "  output['batch_size'] = batch_size\n",
    "  output['data'] = np.zeros((num, batch_size))\n",
    "  \n",
    "  # TODO: implement your inner product forward pass here\n",
    "  # implementation begins\n",
    "  # print(param['w'].shape, param['b'].shape, input['data'].shape)\n",
    "  for batch in range(batch_size):\n",
    "    input_data = input['data'][:,batch]\n",
    "    output['data'][:,batch] = np.dot(param['w'].T, input_data) + param['b']\n",
    "  \n",
    "  # implementation ends\n",
    "\n",
    "  assert np.all(output['data'].shape == (num, batch_size)), 'output[\\'data\\'] has incorrect shape! (error at ip forward)'\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def inner_product_backward(output, input, layer, param):\n",
    "  \"\"\"Fully connected layer backward\n",
    "\n",
    "  Args:\n",
    "    output: a dictionary contains output data and shape information\n",
    "    input: a dictionary contains input data and shape information\n",
    "    layer: one cnn layer, defined in testLeNet.py\n",
    "    param: parameters, a dictionary\n",
    "\n",
    "  Returns:\n",
    "    para_grad: a dictionary stores gradients of parameters\n",
    "    input_od: gradients w.r.t input data\n",
    "  \"\"\"\n",
    "  param_grad = {}\n",
    "  param_grad['b'] = np.zeros(param['b'].shape)\n",
    "  param_grad['w'] = np.zeros(param['w'].shape)\n",
    "  input_od = np.zeros(input['data'].shape)\n",
    "\n",
    "  param_grad['w'] = input['data'].dot(output['diff'].T)\n",
    "  param_grad['b'] = np.sum(output['diff'], axis=1)\n",
    "\n",
    "  input_od = param['w'].dot(output['diff'])\n",
    "  assert np.all(input['data'].shape == input_od.shape), 'input_od has incorrect shape! (error at ip backward)'\n",
    "  assert np.all(param_grad['w'].shape == param['w'].shape), 'w has incorrect shape! (error at ip backward)'\n",
    "  assert np.all(param_grad['b'].shape == param['b'].shape), 'b has incorrect shape! (error at ip backward)'\n",
    "\n",
    "  return param_grad, input_od\n",
    "\n",
    "\n",
    "def get_lr(step, epsilon, gamma, power):\n",
    "  \"\"\"Get the learning rate at step iter\"\"\"\n",
    "  lr_t = epsilon / math.pow(1 + gamma*step, power)\n",
    "  return lr_t\n",
    "\n",
    "\n",
    "def mlrloss(wb, X, y, K, prediction):\n",
    "  \"\"\"Loss layer\n",
    "\n",
    "  Args:\n",
    "    wb: concatenation of w and b, shape = (num+1, K-1)\n",
    "    X: input data, shape = (num, batch size)\n",
    "    y: ground truth label, shape = (batch size, )\n",
    "    K: K distinct classes (0 to K-1)\n",
    "    prediction: whether calculate accuracy or not\n",
    "\n",
    "  Returns:\n",
    "    nll: negative log likelihood, a scalar, no need to divide batch size\n",
    "    g: gradients of parameters\n",
    "    od: gradients w.r.t input data\n",
    "    percent: accuracy for this batch\n",
    "  \"\"\"\n",
    "  (_, batch_size) = X.shape\n",
    "  theta = wb[:-1, :]\n",
    "  bias = wb[-1, :]\n",
    "\n",
    "  # Convert ground truth label to one-hot vectors\n",
    "  I = np.zeros((K, batch_size))\n",
    "  I[y, np.arange(batch_size)] = 1\n",
    "\n",
    "  # Compute the values after the linear transform\n",
    "  activation = np.transpose(X.T.dot(theta) + bias)\n",
    "  activation = np.vstack([activation, np.zeros(batch_size)])\n",
    "\n",
    "  # This rescales so that all values are negative, hence, no overflow\n",
    "  # problems with the exp operation (a single +inf can blow things up)\n",
    "  activation -= np.max(activation, axis=0)\n",
    "  activation = np.exp(activation)\n",
    "\n",
    "  # Convert to probabilities by normalizing\n",
    "  prob = activation / np.sum(activation, axis=0)\n",
    "\n",
    "  nll = 0\n",
    "  od = np.zeros(prob.shape)\n",
    "\n",
    "  nll = -np.sum(np.log(prob[y, np.arange(batch_size)]))\n",
    "\n",
    "  if prediction == 1:\n",
    "    indices = np.argmax(prob, axis=0)\n",
    "    percent = len(np.where(y == indices)[0]) / float(len(y))\n",
    "  else:\n",
    "    percent = 0\n",
    "\n",
    "  # compute gradients\n",
    "  od = prob - I\n",
    "  gw = od.dot(X.T)\n",
    "  gw = gw[0:-1, :].T\n",
    "  gb = np.sum(od, axis=1)\n",
    "  gb = gb[0:-1]\n",
    "  g = np.vstack([gw, gb])\n",
    "\n",
    "  od = theta.dot(od[0:-1, :])\n",
    "\n",
    "  return nll, g, od, percent\n",
    "\n",
    "\n",
    "\n",
    "def sgd_momentum(w_rate, b_rate, mu, decay, params, param_winc, param_grad):\n",
    "  \"\"\"Update the parameters with sgd with momentum\n",
    "\n",
    "  Args:\n",
    "    w_rate: sgd rate for updating w\n",
    "    b_rate: sgd rate for updating b\n",
    "    mu: momentum\n",
    "    decay: weight decay of w\n",
    "    params: original weight parameters\n",
    "    param_winc: buffer to store history gradient accumulation\n",
    "    param_grad: gradient of parameter\n",
    "\n",
    "  Returns:\n",
    "    params_: updated parameters\n",
    "    param_winc_: gradient buffer of previous step\n",
    "  \"\"\"\n",
    "  params_ = copy.deepcopy(params)\n",
    "  param_winc_ = copy.deepcopy(param_winc)\n",
    "\n",
    "  for layerNumber in param_winc_:\n",
    "\n",
    "    param_winc_[layerNumber]['w'] = (mu * param_winc_[layerNumber]['w']) + (w_rate * (param_grad[layerNumber]['w']  + (decay * params[layerNumber]['w'])))\n",
    "    param_winc_[layerNumber]['b'] = (mu * param_winc_[layerNumber]['b']) + (b_rate * param_grad[layerNumber]['b'])\n",
    "\n",
    "    params_[layerNumber]['w'] = params_[layerNumber]['w'] - param_winc_[layerNumber]['w']\n",
    "    params_[layerNumber]['b'] = params_[layerNumber]['b'] - param_winc_[layerNumber]['b']\n",
    "\n",
    "  assert len(params_) == len(param_grad), 'params_ does not have the right length (error at sgd momentum)'\n",
    "  assert len(param_winc_) == len(param_grad), 'param_winc_ does not have the right length (error at sgd momentum)'\n",
    "  return params_, param_winc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-743e3092dcc6>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-743e3092dcc6>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def get_lenet():\n",
    "    \"\"\"Define LeNet\n",
    "\n",
    "    Explanation of parameters:\n",
    "    type: layer type, supports convolution, pooling, relu\n",
    "    channel: input channel\n",
    "    num: output channel\n",
    "    k: convolution kernel width (== height)\n",
    "    group: split input channel into several groups, not used in this assignment\n",
    "    \"\"\"\n",
    "\n",
    "    layers = {}\n",
    "    layers[1] = {}\n",
    "    layers[1]['type'] = 'DATA'\n",
    "    layers[1]['height'] = 28\n",
    "    layers[1]['width'] = 28\n",
    "    layers[1]['channel'] = 1\n",
    "    layers[1]['batch_size'] = 64\n",
    "\n",
    "    layers[2] = {}\n",
    "    layers[2]['type'] = 'CONV'\n",
    "    layers[2]['num'] = 20\n",
    "    layers[2]['k'] = 5\n",
    "    layers[2]['stride'] = 1\n",
    "    layers[2]['pad'] = 0\n",
    "    layers[2]['group'] = 1\n",
    "\n",
    "    layers[3] = {}\n",
    "    layers[3]['type'] = 'POOLING'\n",
    "    layers[3]['k'] = 2\n",
    "    layers[3]['stride'] = 2\n",
    "    layers[3]['pad'] = 0\n",
    "\n",
    "    layers[4] = {}\n",
    "    layers[4]['type'] = 'CONV'\n",
    "    layers[4]['num'] = 50\n",
    "    layers[4]['k'] = 5\n",
    "    layers[4]['stride'] = 1\n",
    "    layers[4]['pad'] = 0\n",
    "    layers[4]['group'] = 1\n",
    "\n",
    "    layers[5] = {}\n",
    "    layers[5]['type'] = 'POOLING'\n",
    "    layers[5]['k'] = 2\n",
    "    layers[5]['stride'] = 2\n",
    "    layers[5]['pad'] = 0\n",
    "\n",
    "    layers[6] = {}\n",
    "    layers[6]['type'] = 'IP'\n",
    "    layers[6]['num'] = 500\n",
    "    layers[6]['init_type'] = 'uniform'\n",
    "\n",
    "    layers[7] = {}\n",
    "    layers[7]['type'] = 'RELU'\n",
    "\n",
    "    layers[8] = {}\n",
    "    layers[8]['type'] = 'LOSS'\n",
    "    layers[8]['num'] = 10\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lenet\n",
    "layers = get_lenet()\n",
    "\n",
    "# load data\n",
    "# change the following value to true to load the entire dataset\n",
    "fullset = True\n",
    "print(\"Loading MNIST Dataset...\")\n",
    "xtrain, ytrain, xval, yval, xtest, ytest = load_mnist(fullset)\n",
    "print(\"MNIST Dataset Loading Complete!\\n\")\n",
    "\n",
    "xtrain = np.hstack([xtrain, xval])\n",
    "ytrain = np.hstack([ytrain, yval])\n",
    "m_train = xtrain.shape[1]\n",
    "\n",
    "# cnn parameters\n",
    "batch_size = 64\n",
    "mu = 0.9\n",
    "epsilon = 0.01\n",
    "gamma = 0.0001\n",
    "power = 0.75\n",
    "weight_decay = 0.0005\n",
    "w_lr = 1\n",
    "b_lr = 2\n",
    "\n",
    "test_interval = 100\n",
    "display_interval = 100\n",
    "snapshot = 5000\n",
    "max_iter = 10000\n",
    "\n",
    "# initialize parameters\n",
    "print(\"Initializing Parameters...\")\n",
    "params = init_convnet(layers)\n",
    "param_winc = copy.deepcopy(params)\n",
    "print(\"Initilization Complete!\\n\")\n",
    "\n",
    "for l_idx in range(1, len(layers)):\n",
    "    param_winc[l_idx]['w'] = np.zeros(param_winc[l_idx]['w'].shape)\n",
    "    param_winc[l_idx]['b'] = np.zeros(param_winc[l_idx]['b'].shape)\n",
    "\n",
    "# learning iterations\n",
    "random.seed(100000)\n",
    "indices = list(range(m_train))\n",
    "random.shuffle(indices)\n",
    "\n",
    "print(\"Training Started. Printing report on training data every \" + str(display_interval) + \" steps.\")\n",
    "print(\"Printing report on test data every \" + str(test_interval) + \" steps.\\n\")\n",
    "\n",
    "startTime = time.clock()\n",
    "for step in range(max_iter):\n",
    "    print(\"current step is: {}\".format(step))\n",
    "    # get mini-batch and setup the cnn with the mini-batch\n",
    "    start_idx = step * batch_size % m_train\n",
    "    end_idx = (step+1) * batch_size % m_train\n",
    "    if start_idx > end_idx:\n",
    "      random.shuffle(indices)\n",
    "      continue\n",
    "    idx = indices[start_idx: end_idx]\n",
    "\n",
    "    [output, cp, param_grad] = conv_net(params,\n",
    "                                          layers,\n",
    "                                          xtrain[:, idx],\n",
    "                                          ytrain[idx], True)\n",
    "\n",
    "    # we have different epsilons for w and b\n",
    "    w_rate = get_lr(step, epsilon*w_lr, gamma, power)\n",
    "    b_rate = get_lr(step, epsilon*b_lr, gamma, power)\n",
    "    params, param_winc = sgd_momentum(w_rate,\n",
    "                           b_rate,\n",
    "                           mu,\n",
    "                           weight_decay,\n",
    "                           params,\n",
    "                           param_winc,\n",
    "                           param_grad)\n",
    "\n",
    "    # display training loss\n",
    "    if (step+1) % display_interval == 0:\n",
    "      print ('training_cost = %f training_accuracy = %f' % (cp['cost'], cp['percent']) + ' current_step = ' + str(step + 1))\n",
    "\n",
    "    # display test accuracy\n",
    "    if (step+1) % test_interval == 0:\n",
    "      layers[1]['batch_size'] = xtest.shape[1]\n",
    "      cptest, _ = conv_net(params, layers, xtest, ytest, False)\n",
    "      layers[1]['batch_size'] = 64\n",
    "      print('test_cost = %f test_accuracy = %f' % (cptest['cost'], cptest['percent']) + ' current_step = ' + str(step + 1) + '\\n')\n",
    "\n",
    "\n",
    "    # save params peridocally to recover from any crashes\n",
    "    if (step+1) % snapshot == 0:\n",
    "      pickle_path = 'lenet.mat'\n",
    "      pickle_file = open(pickle_path, 'wb')\n",
    "      pickle.dump(params, pickle_file)\n",
    "      pickle_file.close()\n",
    "\n",
    "    # print(output[1]['data'].shape, output[2]['data'].shape, output[3]['data'].shape)\n",
    "    if step == 29:\n",
    "      layers[1]['batch_size'] = xtest.shape[1]\n",
    "      output, _ , _ = conv_net(params, layers, xtest, ytest, False)\n",
    "      layers[1]['batch_size'] = 64\n",
    "      printableData = output[2]['data'][:,0]\n",
    "      printableData.shape = [24, 24, 20]\n",
    "      plt.figure()\n",
    "      for num in range(20):\n",
    "        plt.subplot(4, 5, num+1)\n",
    "        plt.imshow(printableData[:,:,num])\n",
    "      plt.show()\n",
    "\n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(startTime-endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers[1]['batch_size'] = xtest.shape[1]\n",
    "output, _ , _ = conv_net(params, layers, xtest, ytest, False)\n",
    "layers[1]['batch_size'] = 64\n",
    "printableData = output[2]['data'][:,0]\n",
    "printableData.shape = [24, 24, 20]\n",
    "plt.figure()\n",
    "for num in range(20):\n",
    "plt.subplot(4, 5, num+1)\n",
    "plt.imshow(printableData[:,:,num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printableData = output[2]['data'][:,0]\n",
    "printableData.shape = [12, 12, 20]\n",
    "\n",
    "plt.figure()\n",
    "for num in range(20):\n",
    "plt.subplot(4, 5, num+1)\n",
    "plt.imshow(printableData[:,:,num])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
