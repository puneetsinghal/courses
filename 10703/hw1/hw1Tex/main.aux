\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{sec:p1}{{}{1}{Problem 1 (13+7=20 pts)}{section*.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  MDP for Problem 1, Part I. The goal is for the agent to have a policy that always leads it on the shortest path to the coffee at coordinate $(3,3)$. In this instance, the agent's current state is $(1,1, N)$ }}{3}{figure.1}}
\newlabel{fig:prob1_shortest_path_domain}{{1}{3}{MDP for Problem 1, Part I. The goal is for the agent to have a policy that always leads it on the shortest path to the coffee at coordinate $(3,3)$. In this instance, the agent's current state is $(1,1, N)$}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces MDP for Problem 1, Part II. The agent starts in state $(1, 1, N)$. This time, the agent has a choice between a small coffee at $(1,6)$ closer to the agent and a large coffee at $(6,6)$ further away from the agent. }}{6}{figure.2}}
\newlabel{fig:1b-mdp}{{2}{6}{MDP for Problem 1, Part II. The agent starts in state $(1, 1, N)$. This time, the agent has a choice between a small coffee at $(1,6)$ closer to the agent and a large coffee at $(6,6)$ further away from the agent}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MDP for Problem 1, Part II. The agent starts in state $(1, 1, N)$. This time, the agent has a choice between a small coffee at $(1,6)$ closer to the agent and a large coffee at $(6,6)$ further away from the agent. }}{7}{figure.3}}
\newlabel{fig:1b-mdp}{{3}{7}{MDP for Problem 1, Part II. The agent starts in state $(1, 1, N)$. This time, the agent has a choice between a small coffee at $(1,6)$ closer to the agent and a large coffee at $(6,6)$ further away from the agent}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Policy iteration, taken from Section 4.3 of Sutton \& Barto's RL book (2018).}}{10}{figure.4}}
\newlabel{fig:policy-iteration}{{4}{10}{Policy iteration, taken from Section 4.3 of Sutton \& Barto's RL book (2018)}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Value iteration, taken from Section 4.4 of Sutton \& Barto's RL book (2018).}}{10}{figure.5}}
\newlabel{fig:value-iteration}{{5}{10}{Value iteration, taken from Section 4.4 of Sutton \& Barto's RL book (2018)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  An example (deterministic) policy for a $4 \times 4$ map of the \texttt  {FrozenLake-v0} environment. L, D, U, R represent the actions up, down, left, right respectively.}}{11}{figure.6}}
\newlabel{fig:prob2_example_policy}{{6}{11}{An example (deterministic) policy for a $4 \times 4$ map of the \texttt {FrozenLake-v0} environment. L, D, U, R represent the actions up, down, left, right respectively}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Example of value function color plot for a $4 \times 4$ map of the \texttt  {FrozenLake-v0} environment. Make sure you include the color bar or some kind of key.}}{12}{figure.7}}
\newlabel{fig:prob3_value_image}{{7}{12}{Example of value function color plot for a $4 \times 4$ map of the \texttt {FrozenLake-v0} environment. Make sure you include the color bar or some kind of key}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Solution of value function color plot for a $4 \times 4$ map of the \texttt  {FrozenLake-v0} environment.}}{13}{figure.8}}
\newlabel{fig:prob3_value_image_solution_for_policy_iteration}{{8}{13}{Solution of value function color plot for a $4 \times 4$ map of the \texttt {FrozenLake-v0} environment}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Solution of value function color plot for a $8 \times 8$ map of the \texttt  {FrozenLake-v0} environment.}}{13}{figure.9}}
\newlabel{fig:prob3_value_image_solution_for_policy_iteration_8X8}{{9}{13}{Solution of value function color plot for a $8 \times 8$ map of the \texttt {FrozenLake-v0} environment}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Solution of value function color plot for a $4 \times 4$ map of the \texttt  {FrozenLake-v0} environment.}}{14}{figure.10}}
\newlabel{fig:prob3_value_image_solution_for_value_iteration}{{10}{14}{Solution of value function color plot for a $4 \times 4$ map of the \texttt {FrozenLake-v0} environment}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Solution of value function color plot for a $8 \times 8$ map of the \texttt  {FrozenLake-v0} environment.}}{15}{figure.11}}
\newlabel{fig:prob3_value_image_solution_for_value_iteration_8X8}{{11}{15}{Solution of value function color plot for a $8 \times 8$ map of the \texttt {FrozenLake-v0} environment}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  Solution of value function color plot for a $4 \times 4$ map of the \texttt  {FrozenLake-v0} environment.}}{18}{figure.12}}
\newlabel{fig:prob3_value_image_solution_for_value_iteration_stochastic}{{12}{18}{Solution of value function color plot for a $4 \times 4$ map of the \texttt {FrozenLake-v0} environment}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Solution of value function color plot for a $8 \times 8$ map of the \texttt  {FrozenLake-v0} environment.}}{19}{figure.13}}
\newlabel{fig:prob3_value_image_solution_for_value_iteration_8X8_stochastic}{{13}{19}{Solution of value function color plot for a $8 \times 8$ map of the \texttt {FrozenLake-v0} environment}{figure.13}{}}
